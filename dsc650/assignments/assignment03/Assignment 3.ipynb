{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Taylor Imhof\n",
    "# Bellevue University | DSC 650"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "#import s3fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# reads zipped json data from path to records variable\n",
    "def get_jsonl_flight_records():\n",
    "    zipped_json_path = 'C:\\\\Users\\\\taylo\\\\OneDrive\\\\Documents\\\\dsc650\\\\data\\\\processed\\\\openflights\\\\routes.jsonl.gz'\n",
    "\n",
    "    with gzip.open(zipped_json_path, 'r') as fin:\n",
    "        records = [json.loads(line) for line in fin.readlines()]\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# def read_jsonl_data():\n",
    "#     s3 = s3fs.S3FileSystem(\n",
    "#         anon=True,\n",
    "#         client_kwargs={\n",
    "#             'endpoint_url': endpoint_url\n",
    "#         }\n",
    "#     )\n",
    "#     src_data_path = 'C:\\\\Users\\\\taylo\\\\OneDrive\\\\Documents\\\\dsc650\\\\data\\\\processed\\\\openflights\\\\routes.jsonl.gz'\n",
    "#     with s3.open(src_data_path, 'rb') as f_gz:\n",
    "#         with gzip.open(f_gz, 'rb') as f:\n",
    "#             records = [json.loads(line) for line in f.readlines()]\n",
    "#\n",
    "#\n",
    "#     return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airline': {'airline_id': 410, 'name': 'Aerocondor', 'alias': 'ANA All Nippon Airways', 'iata': '2B', 'icao': 'ARD', 'callsign': 'AEROCONDOR', 'country': 'Portugal', 'active': True}, 'src_airport': {'airport_id': 2965, 'name': 'Sochi International Airport', 'city': 'Sochi', 'country': 'Russia', 'iata': 'AER', 'icao': 'URSS', 'latitude': 43.449902, 'longitude': 39.9566, 'altitude': 89, 'timezone': 3.0, 'dst': 'N', 'tz_id': 'Europe/Moscow', 'type': 'airport', 'source': 'OurAirports'}, 'dst_airport': {'airport_id': 2990, 'name': 'Kazan International Airport', 'city': 'Kazan', 'country': 'Russia', 'iata': 'KZN', 'icao': 'UWKD', 'latitude': 55.606201171875, 'longitude': 49.278701782227, 'altitude': 411, 'timezone': 3.0, 'dst': 'N', 'tz_id': 'Europe/Moscow', 'type': 'airport', 'source': 'OurAirports'}, 'codeshare': False, 'equipment': ['CR2']}\n"
     ]
    }
   ],
   "source": [
    "# store flight records data using utility function\n",
    "records = get_jsonl_flight_records()\n",
    "print(records[0])\n",
    "one_json = records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    # add validation path\n",
    "    validation_csv_path = results_dir.joinpath('validation-results.csv')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "\n",
    "    with open(validation_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        # assign column names for results.csv\n",
    "        cols = ['row_num', 'is_valid', 'msg']\n",
    "\n",
    "        # create csv writer object\n",
    "        writer = csv.DictWriter(f, fieldnames=cols)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # validate each json record using the json schema\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO: Validate record\n",
    "                jsonschema.validate(instance=record, schema=schema)\n",
    "                # write validation info to results.csv\n",
    "                result = dict(\n",
    "                    row_num = i,\n",
    "                    is_valid = True,\n",
    "                    msg = record\n",
    "                )\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                result = dict(\n",
    "                    row_num = i,\n",
    "                    is_valid = False,\n",
    "                    msg = record\n",
    "                )\n",
    "            finally:\n",
    "                # write the entry to results csv\n",
    "                writer.writerow(result)\n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    # get avro schema into variable\n",
    "    with open(schema_path, 'r') as f:\n",
    "        parsed_schema = fastavro.schema.load_schema(schema_path)\n",
    "\n",
    "    with open(data_path, 'wb') as fout:\n",
    "        fastavro.writer(fout, parsed_schema, records)\n",
    "\n",
    "    # used to check if data was written properly\n",
    "    # with open(data_path, 'rb') as fo:\n",
    "    #     avro_reader = fastavro.reader(fo)\n",
    "    #     for record in avro_reader:\n",
    "    #         print(record)\n",
    "\n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "struct<active: bool, airline_id: int64, alias: string, callsign: string, country: string, iata: string, icao: string, name: string>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36mget_logical_type\u001B[1;34m(arrow_type)\u001B[0m\n\u001B[0;32m     76\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 77\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlogical_type_map\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0marrow_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     78\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 24",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-38-f8995235461c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;31m#     pq.write_table(table, fout)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m \u001B[0mcreate_parquet_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-38-f8995235461c>\u001B[0m in \u001B[0;36mcreate_parquet_dataset\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mtable\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTable\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[0mpq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_table\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparquet_output_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\table.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.Table.from_pandas\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36mdataframe_to_arrays\u001B[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001B[0m\n\u001B[0;32m    391\u001B[0m     metadata = construct_metadata(\n\u001B[0;32m    392\u001B[0m         \u001B[0mdf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_columns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex_column_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpreserve_index\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 393\u001B[1;33m         \u001B[0mtypes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    394\u001B[0m     )\n\u001B[0;32m    395\u001B[0m     \u001B[0mnames\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcolumn_names\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mindex_column_names\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36mconstruct_metadata\u001B[1;34m(df, column_names, index_levels, index_column_names, preserve_index, types)\u001B[0m\n\u001B[0;32m    212\u001B[0m             \u001B[0mfield_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msanitized_name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m         \u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msanitized_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marrow_type\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 214\u001B[1;33m             \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_types\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    215\u001B[0m         )\n\u001B[0;32m    216\u001B[0m     ]\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    211\u001B[0m             \u001B[0marrow_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0marrow_type\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    212\u001B[0m             \u001B[0mfield_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msanitized_name\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 213\u001B[1;33m         \u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcol_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msanitized_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marrow_type\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    214\u001B[0m             \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn_names\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_types\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m         )\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36mget_column_metadata\u001B[1;34m(column, name, arrow_type, field_name)\u001B[0m\n\u001B[0;32m    155\u001B[0m     \u001B[0mdict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m     \"\"\"\n\u001B[1;32m--> 157\u001B[1;33m     \u001B[0mlogical_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_logical_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrow_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[0mstring_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mextra_metadata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_extension_dtype_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\dsc650\\lib\\site-packages\\pyarrow\\pandas_compat.py\u001B[0m in \u001B[0;36mget_logical_type\u001B[1;34m(arrow_type)\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrow_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDecimal128Type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;34m'decimal'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrow_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: struct<active: bool, airline_id: int64, alias: string, callsign: string, country: string, iata: string, icao: string, name: string>"
     ]
    }
   ],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = 'C:\\\\Users\\\\taylo\\\\OneDrive\\\\Documents\\\\dsc650\\\\data\\\\processed\\\\openflights\\\\routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    zipped_json_path = 'C:\\\\Users\\\\taylo\\\\OneDrive\\\\Documents\\\\dsc650\\\\data\\\\processed\\\\openflights\\\\routes.jsonl.gz'\n",
    "\n",
    "\n",
    "    with gzip.open(zipped_json_path, 'r') as fin:\n",
    "        records = [json.loads(line) for line in fin.readlines()]\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.head()\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "\n",
    "    # with gzip.open(zipped_json_path, 'r') as fin:\n",
    "    #     for line in fin:\n",
    "    #         records = [json.loads(line) for line in fin.readlines()]\n",
    "    #\n",
    "    # # create pandas df from records data\n",
    "    # df = pd.DataFrame(records)\n",
    "    # # create parquet table from pandas df\n",
    "    # table = pa.Table.from_pandas(df)\n",
    "    # # write parquet table to output path\n",
    "    # with open(parquet_output_path, 'rb') as f:\n",
    "    #     pq.write_table(table, f)\n",
    "\n",
    "    # write table to parquet output path\n",
    "    # with open(parquet_output_path, 'rb', encoding='utf-8') as fout:\n",
    "    #     pq.write_table(table, fout)\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    pass\n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}